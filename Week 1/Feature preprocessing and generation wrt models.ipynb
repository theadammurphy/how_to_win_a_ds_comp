{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocessing and Generation wrt Models\n",
    "\n",
    "Always have to preprocess features and create new ones from existing ones.\n",
    "\n",
    "Cover:\n",
    "1. Feature preprocessing\n",
    "2. Feature generation\n",
    "3. Their dependence on a model type\n",
    "\n",
    "Will cover: numeric, categorical, datetime, and coordinate features (and missing values).\n",
    "\n",
    "Different kinds of features: binary (0 or 1), numeric, counts, categorical, id (these are unique on each row, they are _not_ numeric features), text.\n",
    "\n",
    "**Why do we care about different features having different types?**\n",
    "There is a strong connection between preprocessing our model and common feature generation methods for each feature type.\n",
    "\n",
    "## Feature Preprocessing\n",
    "\n",
    "Usually we can take the features, fit our fave model and expect it to get great results.\n",
    "\n",
    "Each feature has its own ways of being preprocessed to improve the quality of the model. Choice of preprocessing depends on the model we are going to use.\n",
    "\n",
    "If we OHE, this will create linear boundaries on our model, for example, and thus linear models will perform much better. But RF doesn't need you to OHE variables, it can deal with non-linear decision boundaries just fine.\n",
    "\n",
    "## Feature Generation\n",
    "\n",
    "If we have a linear trend, we can add linear features. E.g. if we are doing sales predictions, we can add in the past weeks as a feature (week 1, 2, 3, etc.) as this will give it a linear thing to grab on to.\n",
    "\n",
    "But a GBDT will use this feature to calculate something like mean target value for each week. Explanation was a bit weird and confusing. BUt basically the idea is that GBDT can't see linear dependencies (as it is just trained on the data immediately beforehand)\n",
    "\n",
    "## Numeric Features\n",
    "\n",
    "We'll look at preprocessing for tree-based and non-tree based. Then we'll look at feature generation.\n",
    "\n",
    "Some models depend on feature scale, some do not. If you look at the decision boundaries between trees and non-trees it is clear that the trees are much harder and more square boundaries. But non-trees are more curved and can often be continuous.\n",
    "\n",
    "If we use linear models, we are always going to use regularization as this gives us a better fit. However, regularization is proportional to feature scale (I went through this in detail for Jedha). Intuitively we can understand that bigger features will be regularized. Plus, optimization techniques can work differently depending on the feature's scale. Thus, we will want to scale our features when using linear models as well (since we will always get a better fit using regularized models).\n",
    "\n",
    "Also, gradient descent methods can go crazy without proper scaling. Thus for NNs we need to apply similar preprocessing techniques as for linear models.\n",
    "\n",
    "Different feature scalings lead to different model quality. In this sense, it is another hyperparameter that you need to optimize.\n",
    "\n",
    "Easiest thing to do is scale all features to be on the same scale e.g. using `MinMaxScaler`. Or, of course, we can apply `StandardScaler`. \n",
    "\n",
    "After either normalization or standardization, each feature will have a similar level of impact on non-treebased models. Or rather, each will have the opportunity to have the same amount of impact. Obviously some will be better than others.\n",
    "\n",
    "Note for KNN: we kow that the bigger a feature is, the more important it will be for KNN. So, we can boost our best features by actually making those scales _larger_. Very clever!\n",
    "\n",
    "### Outliers\n",
    "\n",
    "This is especially important for linear models.\n",
    "\n",
    "To protect linear models from outliers, we can clip features between two chosen values of lower bound and upper bound. We choose them as some percentile of that feature e.g. the 1st and 99th percentiles. This is a well known practice for financial data and is called winsorization (I did this with Evan!)\n",
    "\n",
    "### Rank\n",
    "\n",
    "The rank transformation sets spaces between proper assorted values to be equal (huh???). This can be better than MinMaxScaler if we have outliers as rank will move the values closer together. I guess it just transforms the features to be 0, 1, 2, 3, 4 etc.\n",
    "\n",
    "Linear models, KNN and NNs can benefit from this additional kind of transformation if we have no time to handle outliers manually.\n",
    "\n",
    "We can do `from scipy.stats import rankdata` to use it. Note that to apply it to test data, you need to store the creative mapping from feature values to their rank values. Or, alternatively, you can concatenate train and test data before applying the rank transformation to get the proper ranking. This would obviously not work well in the real world!\n",
    "\n",
    "\n",
    "### Transforms\n",
    "\n",
    "These are especially helpful for non-tree based models and especially NNs.\n",
    "\n",
    "Log and sqrt transforms.\n",
    "\n",
    "np.log(1+x) and np.sqrt(1+x)\n",
    "\n",
    "Note that both need to have positive feature values and that we will shift everything by 1 so that we don't have any 0s in the dataset.\n",
    "\n",
    "Both can be useful as they bring values that are too big closer to the feature's average value. Plus the values near zero become more distinguishable.\n",
    "\n",
    "Using these can make a big difference to your model's performance!\n",
    "\n",
    "### Important Note\n",
    "\n",
    "Sometimes it is beneficial to train a model on concatenated dataframes produced by different preprocessings. Or to mix model training with differently preprocessed datasets. Linear models, KNN and NNs can benefit hugely from this. \n",
    "\n",
    "Now we have discussed numeric feature preprocessing, how model choice impacts it and what the most commonly used preprocessing methods are.\n",
    "\n",
    "## Feature Generation\n",
    "\n",
    "This is creating new features using knowledge about the features and the task. It makes model training more simple and effective.\n",
    "\n",
    "Sometimes we can engineer them using prior knowledge and logic. Sometimes we have to dig into the data, create and check hypotheses and use this derived knowledge and our intuition to derive new features.\n",
    "\n",
    "Having prior knowledge is good. But obvs is not something we can rely on. Being able to do solid EDA and find new and better features is what makes a good competitor a great one. They will go over how to do this in detail in future videos.\n",
    "\n",
    "One example is if we have real estate data and have 2 features: squared area, and price, we can combine these and make a new feature price per meter squared which is just the former divided by the latter.\n",
    "\n",
    "Another is if we have horizontal distance to a point and vertical distance to a point, we may as well add the direct distance to the point by using pythagoras to help us!\n",
    "\n",
    "By adding features that are addition, subtraction, multiplication or division of other features combined, we  are helping every single one of our models. Even GBDTs would benefit from adding some of this stuff as they would struggle to do these kinds of calculations themselves.\n",
    "\n",
    "### Fractional Prices\n",
    "\n",
    "Wow ok so this is cool.\n",
    "\n",
    "If one of our features is price, we can create a new feature which is `fractional_part` which just takes the value after the decimal point. This lets the model see if peoples' perception of price impacts what we are modelling.\n",
    "\n",
    "We can find similar patterns in tasks that require distinguishing between a human and a robot. Humans are irrational, robots (at least for now) are not. For example, if we have auction financial data, we may observe that people tend to set round numbers as prices. Likewise, if we are trying to find spambots on social media sites, we can be sure that no human has ever read loads of messages each with an exact interval of one second.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The distinction is between tree and non-tree models. The latter can be heavily impacted by feature scaling and thus all numeric features need to be preprocessed.\n",
    "- Most often preprocessing techniques:\n",
    " - MinMaxScaler to [0, 1]\n",
    " - StandardScaler to mean 0 and std 1\n",
    " - Rank - sets spaces between sorted values to be equal i.e. ranks values from 0, 1, 2, 3...\n",
    " - np.log(1+x) and np.sqrt(1+x)\n",
    "- Feature generation is powered by an understanding of the data.\n",
    "\n",
    "## Categorical and Ordinal Features\n",
    "\n",
    "Ordinal features are _ordered_ categorical features. In the Titanic dataset the feature `Pclass` is ordinal and has three values: 1, 2, 3. Class 1 is 1st class and is more expensive than 3rd class.\n",
    "\n",
    "Note that ordinal features differ from numeric features. \n",
    "\n",
    "If `Pclass` was numeric, we could say that the difference between the first and the second class is equal to the difference between the second and the third class. But, because `Pclass` is ordinal, we don't know which difference is bigger.\n",
    "\n",
    "The example of class seems a bit odd to me. But if we think about the earthquake damage prediction competition we see that we have ordinal classification task (I made this up but it makes sense) and we have 1 being little damage, 2 moderate and 3 being almost total destruction (should there not be one that is 'quite high'?!). So, I'd say the difference between 1-2 is _smaller_ than between 2-3.\n",
    "\n",
    "Ordinal feature examples:\n",
    "- Driver's license type: A, B, C, D\n",
    "- Education: kindergarten, school, undergraduate, masters, doctoral\n",
    "\n",
    "The categories are sorted in an increasingly complex order which can prove to be useful.\n",
    "\n",
    "The simplest way to encode categorical features is to map its unique values to different numbers... (I would have thought that this introduces and ordering that we don't want??).\n",
    "\n",
    "This is called Label Encoding.\n",
    "\n",
    "It is fine to use Label Encoding for trees as they can split features and extract most of the useful values in categories on its own. This makes sense as it can just split the feature anywhere it wants but the other models would have to make a nice line around it which is not really possible.\n",
    "\n",
    "Some situations where it is better to use label encoding rather than OHE for tree based models:\n",
    "- When the number of categorical features in the dataset is huge\n",
    "- When we can create a label encoder that assigns close labels to similar (in terms of target) categories. (this was taken from the quiz and not explained at all either implementation or theory in the videos...)\n",
    "- When categorical feature is ordinal obvs makes sense to use label encoding (same is true if the model is linear!)\n",
    "\n",
    "When OHE can be better than Label:\n",
    "- If the target dependence on the label encoded feature is very non-linear i.e. values that are close to each other in the label encoded feature correspond to target values that aren't close. OHE gives nice clear distinct feature boundaries that our tree can make use of. If a feature is important, a tree would try to make a lot of splits and select each feature's value in a category on its own. But because  trees are built in a greedy way, it can be hard to select one important value in a label encoded vector. This won't be a problem if you use OHE.\n",
    "\n",
    "Note for linear models it is not necessarily always best to OHE (since if the cat feature is ordinal, it will need to be label encoded).\n",
    "\n",
    "Note: if the number of cat features in the dataset is huge, if you OHE you will take up a lot of memory (so use sparse matrcies) and if building trees you could have a situation where the numerical features are hardly ever used to build trees (since a random subset of features is usually chose). You can change this by modifying how many features each tree can use. \n",
    "\n",
    "Note though that it is not necessarily better to use label encoding over one-hot. It will be less computationally expensive but better results are not guaranteed (duh! Not much is guaranteed in ML).\n",
    "\n",
    "But non-tree based models won't be able to use this feature effectively if it has been label encoded.\n",
    "\n",
    "If we have a categorical feature that is not already a number, e.g. given by letters, we need to transform it into numbers before we can use it.\n",
    "\n",
    "We can apply encoding in:\n",
    "- Alphabetical/sorted order - [S, C, Q] -> [2, 1, 3] - `sklearn.preprocessing.LabelEncoder`\n",
    "- Order of appearance - [S, C, Q] -> [1, 2, 3] - `pandas.factorize` - can make sense if the data is sorted in a meaningful way beforehand.\n",
    "\n",
    "### Frequency Encoding\n",
    "\n",
    "We can map values to their frequencies! Now this is cool.\n",
    "\n",
    "```python\n",
    "encoding = titanic.groupby('Embarked').size()\n",
    "encoding = encoding / len(titanic)\n",
    "titanic['enc'] = titanic.Embarked.map(encoding)\n",
    "```\n",
    "\n",
    "Very clever. I would not have known how to do this!\n",
    "\n",
    "This is helpful for both tree and non-tree models. If frequency of a category is correlated with the target value, a linear model will utilize this dependency. And, for the same reason, a tree model will need to use less splits.\n",
    "\n",
    "This preserves info about the values distribution and can help both linear and tree models.\n",
    "\n",
    "Note: if you have multiple categories with the same frequency, they will not be distinguishable in this new feature. So we could apply a rank operation here to deal with such ties using `from scipy.stats import rankdata`.\n",
    "\n",
    "There are other ways to do label encoding and he encourages us to be creative when constructing them.\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "For non-tree based models we cannot use label encoding. So, we use one-hot encoding instead. Note another benefit of this is that the features are already scaled (since the min is 0 and the max is 1 for each column). \n",
    "\n",
    "Note: if you have a few numeric features and hundreds of one-hot encoded features, it can become difficult for tree-based methods to use to the first ones efficiently (this is the situation I am in atm with the earthquake challenge). Tree models will slow down and not always improve results.\n",
    "\n",
    "Note that if our categories have loads of unique values, we may add too many new columns with few non-zero values and now our dataset is sparse. My personal opinion on what 'too many' is going to be learned through trail and error. But either way, it will make sense to store our data as sparse matrices if this happens. This way, we only store non-zero elements of our array and thus save a lot of memory.\n",
    "\n",
    "Going with sparse matrices makes sense if the number of non-zero values is _far less than half of all the values_. Sparse matrices are often useful if we work with a lot of categorical features or text data. Most libraries can work with sparse matrices directly.\n",
    "\n",
    "## Categorical Feature Generation\n",
    "\n",
    "One of the most useful examples of feature generation is feature interaction between several categorical features. This is usually useful for non-tree based models. Especially helpful for linear models and KNNs.\n",
    "\n",
    "If the target depends on both the sex and the class, linear model could adjust its predictions for every possible combination of these two features and get a better result. But how can we make this happen?\n",
    "\n",
    "We could do this by concatenating strings from both columns and one-hot encoding the results i.e. we now have columns 1male, 1female, 2male, 2female, 3male, and 3female. Now our model can find the optimal coefficient for every interaction and improve.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Ordinal features are sorted in some meaningful order\n",
    "- Label encoding maps categories to numbers (this does not automatically mean that the categories are ordered, a tree based model can handle label encoded features no problem but for non-tree based, we must use one-hot encoding).\n",
    "- Frequency encoding maps categories to their frequencies\n",
    "- Label and frequency encodings are often used for tree-based models\n",
    "- One-hot encoding is often used for non-tree based models\n",
    "- Interactions of categorical features can help linear models and KNN\n",
    "\n",
    "## Datetime and Coordinates\n",
    "\n",
    "These differ significantly from numeric and categorical features. We can infer the meaning of both of these very easily and thus can come up with some specific ideas about feature generation.\n",
    "\n",
    "### Datetime\n",
    "\n",
    "It is not just year. It can also include day, week, and (of course) time.\n",
    "\n",
    "Can be divided into two broad categories:\n",
    "1. Time moments in a period (periodicity)\n",
    "2. Time since a particular event\n",
    "\n",
    "**Periodicity**\n",
    "Day number in week, month, season, year, second, minute, hour.\n",
    "Useful to capture repetitive patterns in the data. If we know about non-common periods that influence the data, we can add them as well. For example, if patients take medicine once every 3 days, we can consider this a special time period.\n",
    "\n",
    "**Time Since**\n",
    "- Row-independent moment e.g. since 00:00:00 UTC 1 January 1970\n",
    "- Row-dependent important moment e.g. number of days left until next holidays/time passed after last holiday or since last sales campaign (this seems v useful if predicting sales!). Or even the number of days left until these events.\n",
    "\n",
    "So instead of just date, we have the week day (as a number 0-7), day number since start of 2014 (0-364), is_holiday (binary), days_till_holidays (countdown until the next holiday), number of sales.\n",
    "\n",
    "Woah yeah we can do SO MUCH with date data.\n",
    "\n",
    "**Difference Between Dates**\n",
    "Sometimes we have several datetime columns in our data. Tdddhe most straightforward idea here is to subtract one feature from another. Or perhaps subtract the features we have just generated.\n",
    "\n",
    "One example could be calculating the difference between the date someone last purchased something vs. the time we called them. Perhaps big differences lead to more churn?\n",
    "\n",
    "Note that once you've processed datetime, you will usually get numeric features (time passed since 2000) or categorical features (day of week). And thus, now _these_ features will need to be treated accordingly with the topics above. \n",
    "\n",
    "## Coordinate Data\n",
    "\n",
    "Let's assume we are doing house price prediction problem.\n",
    "\n",
    "Generally you want to calculate distances to important points on the map. For example, could add distance to nearest shop, to the best school in the neighborhood and so on. If you do not have this, you can extract interesting points on the map from all data you have available to you.\n",
    "\n",
    "For example, you can divide your map into grid squares and, within each square, find the most expensive flat. Then, for every other object in this square, add the distance to that flat.\n",
    "\n",
    "Or you can organize data points into clusters and use centers of clusters as the important points. \n",
    "\n",
    "Or find special areas e.g. those with v old buildings and add distance to this one.\n",
    "\n",
    "Can also calculate aggregated stats for objects surrounding an area e.g. number of lets around a particular point which can then be interpreted as areas of popularity. Or add mean realty price which indicates how expensive the area is around selected points.\n",
    "\n",
    "Both distances and aggregated statistics are often useful in tasks with coordinates.\n",
    "\n",
    "Another killer idea: if you are using decision trees, it may make sense to rotate your coordinates slightly so that you can get perfectly horiztonal and linear decision boundaries (since trees work best with those). It can be hard to know what rotations to make, so you can test different ones and see which performs best. Common are 45 or 22.5 rotations.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Summary of most frequent methods used for feature generation from datetime and coordinates\n",
    "\n",
    "Datetime\n",
    "- Periodicity\n",
    "- Time since row-independent/dependent event\n",
    "- Difference between dates\n",
    "\n",
    "Coordinates\n",
    "- Interesting places from train/test data or additional data\n",
    "- Centers of clusters\n",
    "- Aggregated statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tabular]",
   "language": "python",
   "name": "conda-env-tabular-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
