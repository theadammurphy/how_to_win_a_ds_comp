{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "Often have to deal with missing values. They can be not numbers, empty strings or outliers like -999.\n",
    "\n",
    "Sometimes they can contain useful information themselves. What was the reason the missing value occurred?\n",
    "\n",
    "We can see from the distributions if the data collectors have used something other than NaN to record missing values. We may see, on a histogram, a small cluster at a specific number.\n",
    "\n",
    "So, **missing values can be hidden from us**!!! This is very important to realize.\n",
    "\n",
    "## Imputation\n",
    "\n",
    "- Replace NaN with some value outside fixed value range e.g. -999, -1. Useful as trees can treat missing values as a separate category. But linear models and NNs would suffer.\n",
    "- Mean, median - useful for simple linear models or NNs. And then for trees, it will be hard (if not impossible) to select an object which had a missing value in the first place.\n",
    "- Reconstruct value\n",
    "\n",
    "We can add a `isnull` feature which is binary and indicates if that particular feature is NaN or not. So we can still get the benefits of being able to see if a value is missing or not and also the benefits of having no missing values (after imputation). \n",
    "\n",
    "Downside is that we will double the number of columns in the dataset...\n",
    "\n",
    "If we have time series data it is not unreasonable to reconstruct the missing values. e.g. if have time series temperature data for the start and end of the month but not the middle, we can approximate the missing value using something like KNN. \n",
    "\n",
    "But this nice opportunity is unlikely to happen very often. \n",
    "\n",
    "In most scenarios, the rows in our data set are independent and we usually will not find any proper logic to reconstruct them.\n",
    "\n",
    "### Concerns\n",
    "\n",
    "If we generate new features from columns with missing values, we need to be careful. If we impute missing values and then use that column to generate new feature columns, these new columns will contain the assumptions we made when we imputed the missing values. \n",
    "\n",
    "Usually we don't have the time to perform super precise interpolation of missing values.\n",
    "\n",
    "Will look at this more when we do advanced feature engineering. But example is if we have categorical column and numeric column and want to combine them so that the new categorical_encoded column is the mean of all the numeric values from each category. If we encode missing values in the numeric column with -999 this will skew the categorical encoding massively. If one category has a lot of missing values, the categorical encoded feature will be massively skewed towards -999.\n",
    "\n",
    "The same thing happens if we fill missing values with the mean or median of the feature. But one positive here is that it will skew it to the mean or median and not some value outside the range. Also, could definitely argue, that the mean is massively impacted by huge numbers but the mean/median would not be massive outliers and so would not cause such big skew related problems?\n",
    "\n",
    "Either way, this imputation can definitely screw up the feature we are constructing.\n",
    "\n",
    "The way to handle it (in this case) is to ignore missing values when calculating the mean for each category.\n",
    "\n",
    "**Be very careful with early NaN imputation if you want to generate new features!!**\n",
    "\n",
    "### Other ideas\n",
    "\n",
    "- Sometimes we can treat outliers as missing values if they are particularly extreme or don't make sense. E.g. if classifying songs and there are some that apparently composed before Ancient Rome or after 2050.\n",
    "\n",
    "Sometimes beneficial to change the categories which are present in the test data but not in the training data. The model could not learn anything about this category during training and so will not be able to infer anything from it when making predictions. So you can actually go to the test data and manually change any categories that appear there to those that appear in the training data.\n",
    "\n",
    "Unsupervised encodings of categorical features can help e.g. change categories to its frequency. Thus we can treat categories we've never seen before based on their frequency. So we can even work with unseen categories on our test data. Pretty cool!\n",
    "\n",
    "We can create a column `categorical_encoded` which is the number of times each category appears across _both_ train and test data. \n",
    "\n",
    "## Summary\n",
    "\n",
    "- Choice of NaN fill method depends on the situation\n",
    "- Usually way to deal with them is to replace with -999, mean, or median (the former lets trees create a separate category for NaNs automatically but linear models would get put off)\n",
    "- Missing values may already be replaced with something by the competition organizers - can find the rows by browsing histograms\n",
    "- Binary feature `isnull` can be beneficial for each column (but doubles the number of columns in the dataset!)\n",
    "- In general, avoid filling NaNs before feature generation (it can massively decrease the usefulness of the features)\n",
    "- XGBoost (and LGBM?) can handle NaNs itself, so you don't even need to impute them!! Woo!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
