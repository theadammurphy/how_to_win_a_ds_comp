{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQciYSbtd-HP"
   },
   "source": [
    "# Competition Mechanics\n",
    "\n",
    "Each competition has the following competition elements:\n",
    "\n",
    "* Data - read the description. Sometimes you can use more data than they give you (but need to check the rules)\n",
    "* Model - what we build. It transforms data into answers. Should produce the best possible prediction and be reproducible\n",
    "* Submission - usually we just submit our predictions, they don't care about the actual model. Some (cool) competitions let you submit your code as well. There will usually be a sample submission file that we can look at\n",
    "* Evaluation - a function that takes in `(predictions, right_answers)` and returns a score saying how good the predictions are. But usually we don't really care about the score but rather our relative performance to other competitors.\n",
    "* Leaderboard - usually shows your best score and your position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgrrufycg_Jq"
   },
   "source": [
    "# Real World ML Pipeline\n",
    "\n",
    "It's a complicated process including:\n",
    "\n",
    "1. Understanding the business problem\n",
    "2. Problem formalization\n",
    "2. Data collecting\n",
    "3. Data preprocessing\n",
    "4. Modelling - which model is appropriate? How to select the best model?\n",
    "5. Way to evaluate model in real life\n",
    "6. Way to deploy the the model\n",
    "7. Monitor new performance and train on new data\n",
    "8. Periodically revise your understanding of the problem and go through the cycle again and again\n",
    "\n",
    "# Competition ML Pipeline\n",
    "\n",
    "All the formualization and evaluation parts are already done. And we don't have to deploy the model. So, it's just\n",
    "\n",
    "1. Data preprocessing\n",
    "2. Modelling\n",
    "\n",
    "BUT sometimes you need to understand the business problem to get insights or generate new features.\n",
    "\n",
    "And sometimes you are allowed to use external data. And then data collection becomes a crucial part of the solution.\n",
    "\n",
    "Often the hardest part is problem formalization and choosing target metric.\n",
    "\n",
    "Sometimes you need to be wary of how complex your model is (perhaps so that businesses can deploy them in future) but usually not the case.\n",
    "\n",
    "As competitiors _the only thing that matters is the target metric value_. Speed, complexity, memory consumption, all of these are irrelevant. We just care about getting the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8X_GSICIiXVD"
   },
   "source": [
    "# Competition Philosophy\n",
    "\n",
    "Not just about algorithms. It's about data and making things work. \n",
    "\n",
    "Everyone can and will tune classic approaches. We need some insight to win. These insights are usually more useful than a deftly tuned ensemble.\n",
    "\n",
    "Somtimes there is no ML (shock horror!).\n",
    "\n",
    "Do not limit yourself. The only thing we care about is the target metric. It's ok to use heuristics and manual data analysis. Doesn't be afraid of super complex solutions or advanced feature engineering or doing HUGE calculations. Totally fine to do these things. Use everything you can to get as much juice as you can from your models. \n",
    "\n",
    "Be creative. There are no rules. You can modify or hack an existing algorithm or even design a completely new one! Don't be afraid of reading source code or changing them.\n",
    "\n",
    "Enjoy them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sl5Pojxk6jy"
   },
   "source": [
    "# Families of ML Algorithms\n",
    "\n",
    "- Linear - good for sparse, high-dimensional data.\n",
    " - Sklearn\n",
    " - Vowpal Wabbit both great for linear models\n",
    "- Tree-based - decision tree, random forest, GBDT. Divides space into boxes (as it creates splits along the feature axes). Great for tabular data. In almost every competition, winners use this approach. Hard to capture linear dependencies though as it requires a lot of splits\n",
    " - Sklearn good implementation of RandomForest\n",
    " - Use XGBoost and LightGBM for gradient boosting due to higher speed and accuracy\n",
    "- kNN - features based on kNN are often very informative. Note that square distance makes sense in small dimensions but not really in higher ones. So, you may have to use other metrics.\n",
    " - Sklearn - variety of built-in distance functions and you can use your own\n",
    "- Neural Networks - produce a smooth separating curve (in contrast to decision trees).\n",
    " - Explore TensorFlow playground to get a more intuitive understanding of how NNs work. Loads of frameworks\n",
    " - TensorFlow/Keras\n",
    " - PyTorch (the instructor perfers PyTorch but obvs I know both are fine).\n",
    "\n",
    "## No Free Lunch Theorem\n",
    "\n",
    "There is no method that outperforms all others for all tasks. \n",
    "\n",
    "Some models are better suited for certain tasks.\n",
    "\n",
    "Or rather, for every method, we can construct a task for which this method will not be the best.\n",
    "\n",
    "Each method relies on some assumptions about the data/task. So, we cannot win every competition with just a single algorithm. We need a variety of algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KosbizJaut08"
   },
   "source": [
    "## RandomForest vs. ExtraTrees\n",
    "\n",
    "RandomForest is built by training many decisions trees on a different bootstrapped sample of the training data (thus each tree is trained on effectively a different, but similar dataset). Each tree can only use sqrt(num_features) to make its decisions. So it is using (slight) different data and can only use a (random) subsample of the total number of features to decide. Then we combine the results of these classifiers to get our final answer. \n",
    "\n",
    "ExtraTrees is built by training many decisions trees on the same training data. Again, it can only use sqrt(num_features) to make its decision. But it makes its split at a completely random point in each feature. This is in stark contrast to RandomForests that make the best split they can based on the features they are working with thanks to the 'criterion' they are trying to optimize e.g. gini or entropy.\n",
    "\n",
    "GradientBoosting builds trees one at a time where each new tree helps to correct errors made by the previously trained tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ6jOVZbjTFZ"
   },
   "source": [
    "# Software/Hardware Requirements\n",
    "\n",
    "Most competitions (except image-based) can be solved on:\n",
    "\n",
    "- High-level laptop\n",
    "- 16+ GB RAM\n",
    "- 4+ cores \n",
    "\n",
    "Quite good setup:\n",
    "- Tower PC\n",
    "- 32+ GB ram\n",
    "- 6+ cores\n",
    "\n",
    "(This setup is what the course instructor uses)\n",
    "\n",
    "You can get Mac 13\" old one up to 32GB memory, the 16\" goes up to 64GB!! The new Macbook Pro only goes up to 16GB memory atm but has 8 cores.\n",
    "\n",
    "**Really important things**\n",
    "- **RAM** - if you can keep data in memory, everything will be much easier. So the more RAM you have, the better. 64GB should be more than enough (but some prefer 128GB or even more!)\n",
    "- **Cores** - the more cores you have the more (or faster) experiments you can run (sometimes even 32 is not enough...)\n",
    "- **Storage** - SSD is *cruicial* if you work with images or big datasets with a lot of small pieces. Especially important for training NNs on large number of images.\n",
    "\n",
    "So now I am seeing some massive benefits of doing NLP over CV, namely it will be cheaper for me to learn as I will not have to work with such massive datasets!\n",
    "\n",
    "Obvs can just rent all of this from AWS, GCP, or Azure.\n",
    "\n",
    "Note AWS Spot Option. This lets you bid on unused instances which can lower your cost significantly!\n",
    "\n",
    "Your spot instance runs whenever your current bid exceeds the market price. Generally, it's much cheaper than other options. But there is the risk that your bid will fall below market price and your session will be terminated.\n",
    "\n",
    "Note that Jupyter notebooks allow you to work remotely. It is exactly the same working in a Jupyter notebook on AWS as it is running in your local machine, you just go through the setup differently (by SHH'ing into them I think).\n",
    "\n",
    "Very important things to note:\n",
    "- [Vowpal Wabbit](https://vowpalwabbit.org/) - blazing speed and handle really large datasets which don't fit into memory\n",
    "- [Libfm](https://github.com/srendle/libfm) and [Libffm](https://github.com/ycjuan/libffm) implement different types of optimization machines and often used for sparse data like click-through rate prediction\n",
    "- [rgf](https://github.com/RGF-team/rgf) - an alternative to base methods which he suggests we use in ensembles\n",
    "\n",
    "Being honest I have _no idea_ what these things do (even after reading about them on Github), nor would I have any idea how to implement them. This must be how people feel when they look at Python code lol.\n",
    "\n",
    "The [Final Project For The Course](https://www.kaggle.com/c/competitive-data-science-final-project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKpIrX4od1Ir"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO584hYamzDtuOfaN4qi8YM",
   "collapsed_sections": [],
   "name": "Week 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
