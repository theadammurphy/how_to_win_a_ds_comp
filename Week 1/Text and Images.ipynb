{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "important-people",
   "metadata": {},
   "source": [
    "# Text and Image Data\n",
    "\n",
    "If text/images are in addition to our tabular data, we obvs need to pull some useful stuff out of them.\n",
    "\n",
    "Sometimes we will use the extracted features _with_ the tabular data and other times independently and then ensemble the results together at the end. This is discussed in the ensemble section.\n",
    "\n",
    "# Text\n",
    "\n",
    "There are two main ways to extract features from text:\n",
    "1. Bag of Words\n",
    "2. Embeddings (Word2Vec)\n",
    "\n",
    "## Bag of Words\n",
    "\n",
    "New column for each unique word from the data, then count the number of occurences of each word (can also go simpler and just OHE instead of counting).\n",
    "\n",
    "Use: `sklearn.feature_extraction.text.CountVectorizer`.\n",
    "\n",
    "Remember that KNN, NNs and linear models depend on feature scale (oh this is perhaps why Chollet just OHE'd the words in the basic example so that he didn't have to worry about scaling). So we want to post-process them to avoid these scaling issues.\n",
    "\n",
    "Aims:\n",
    "- Make samples more comparable\n",
    "- Boost more important features and decrease scale of useless ones\n",
    "\n",
    "Methods:\n",
    "1. **Term Frequency** - Normalize sum of values per row (i.e. per piece of text). Thus, count the frequency of words per text sample\n",
    "```python\n",
    "tf = 1 / x.sum(axis=1)[:, None]\n",
    "x = x * tf\n",
    "```\n",
    "2. **Inverse Document Frequency** - Normalize data column wise to boost the more important features. Normalize by the inverse fraction of documents that contain this word. Thus if a word is in every document, e.g. 'a', it will get a very low score\n",
    "```python\n",
    "idf = np.log(x.shape[0] / (x > 0).sum())\n",
    "x = x * idf\n",
    "```\n",
    "By taking the log, we decrease the significance of widespread words (as it pulls them closer to 0).\n",
    "\n",
    "Use: `sklearn.feature_extraction.text.TfidfVectorizer`.\n",
    "\n",
    "Lots of TFiDF variants you should try. No free lunch.\n",
    "\n",
    "### N-Grams\n",
    "\n",
    "Add not just columns corresponding to words but also corresponding to N subsequent words (or sequences of chars).\n",
    "\n",
    "Apparently for N=1 we will have 28 columns (not sure why though since there are 26 letters in the alphabet). Perhaps they include ! and ?\n",
    "\n",
    "Note that for N=2 we have 28 * 28 = 784 possible combinations and thus columns.\n",
    "\n",
    "Benefits of char N-grams:\n",
    "- Sometimes better and more efficient to have every possible char N-gram as a feature instead of having a feature for each unique work from the dataset.\n",
    "- Can handle unseen words\n",
    "\n",
    "Use: `sklearn.feature_extraction.text.CountVectorizer(Ngram_range, analyzer)`\n",
    "\n",
    "- Ngram_range - sets number of n-grams to include\n",
    "- Analyzer - change from word to char n-grams.\n",
    "\n",
    "### Other preprocessing\n",
    "\n",
    "1. Lowercase\n",
    "2. Lemmatization\n",
    "3. Stemming\n",
    "4. Stopwords\n",
    "\n",
    "Without **lowercase** we would get multiple columns for the same words and this is inefficeint. `CountVectorizer` does lowercase by default.\n",
    "\n",
    "**Lemmatization and stemming**\n",
    "- I had a car --> I have car\n",
    "- We have cars --> We have car\n",
    "\n",
    "We unify very similar words.\n",
    "\n",
    "*Stemming* - more heuristic, chop off word endings\n",
    "democracy, democratic, democratization --> democr\n",
    "\n",
    "*Lemmatization* - more precise, need knowledge of vocab and morphological analysis of words\n",
    "democracy, democratic, democratization --> democracy\n",
    "\n",
    "*The Difference*\n",
    "- If we use stemming on 'saw' we get 's'\n",
    "- If we use lemmatization on 'saw' we get 'see' or 'saw' depending on the context (verb or noun)\n",
    "\n",
    "**Stopwords**\n",
    "Unimportant words that occur a lot in language\n",
    "- Articles/prepositions\n",
    "- Very common words\n",
    "\n",
    "Most languages have pre-defined lists of stopwords you can find online or from NLTK (Natural Language ToolKit library for Python).\n",
    "\n",
    "Use: `sklearn.feature_extraction.text.CountVectorizer(max_df)`\n",
    "\n",
    "- max_df - ignore terms that have a higher document frequency strictly higher than the given threshold (e.g. if a word is in 90%+ of the documents, it's probably not very helpful for our task).\n",
    "\n",
    "This is the classical feature preprocessing pipeline for text.\n",
    "\n",
    "## Embeddings (Word2Vec)\n",
    "\n",
    "Vector representations of words and text but more precise and concise than before.\n",
    "\n",
    "Converts each word to a vector in a sophisticated space (several hundred dimensions). \n",
    "\n",
    "king + woman - man = queen\n",
    "\n",
    "Super amazing.\n",
    "\n",
    "There are various embeddings that already exist that we can use:\n",
    "- **Words** - Word2Vec, Glove (Global vector for word representation), FastText, etc.\n",
    "- **Sentences** - Doc2Vec etc.\n",
    "\n",
    "Note that embeddings for words and sentences are different. Could take mean/sum of word vectors or use something else like Doc2vec. Check both approaches and select the best. \n",
    "\n",
    "Training. word2vec takes ages. So use pretrained ones\n",
    "\n",
    "### BOW vs. W2V\n",
    "\n",
    "1. BOW\n",
    " 1. Very large vectors\n",
    " 2. Meaning of each value in vector is known\n",
    "2. Word2vec\n",
    " 1. Relatively small vectors\n",
    " 2. Values in vector can be interpreted only in some cases\n",
    " 3. The words with similar meanings often have similar embeddings\n",
    " \n",
    "Usually both methods give quite different results. So can benefit from using both and seeing which works best. \n",
    "\n",
    "# Images\n",
    "\n",
    "CNNs give a compressed representation of the image. \n",
    "\n",
    "Each CNN has many layers. There is the output from the final layer but also output from the inner layers. We will call inner layer output: **descriptors**.\n",
    "\n",
    "The descriptors from later layers are good at solving tasks similar to the one the (original) network was trained on.\n",
    "\n",
    "Descriptors from earlier layers are more task independent information.\n",
    "\n",
    "E.g. if using a model trained on ImageNet, you can use the last layers for some car model classification task. But it would struggle on medical specific tasks e.g. X-ray scans. Better then to use earlier layers in the network (or even retrain your network from scratch).\n",
    "\n",
    "Want to look for a pretrained model that was trained on data similar to what you have in the exact competition.\n",
    "\n",
    "We will process a pre-trained model to make it suit our needs more, this is called **fine-tuning**.\n",
    "\n",
    "Fine-tuning, especially for small datasets is usually better than training a standalone model on descriptors or training a network from scratch. \n",
    "\n",
    "Training is better than fine-tuning because we can train all network parameters. And thus extract more effective image representations.\n",
    "\n",
    "But fine-tuning is better than training from scratch if we have too little data or if the task we are solving is similar to the task it was trained on (can use knowledge already encoded in the network's parameters). Better results and faster training.\n",
    "\n",
    "One example:\n",
    "Re-train VGG-16. It was originally trained on 1,000 images. But we can replace the last layer with a 4-output layer (as it was a multiclass classification problem with 4 categories), then retrain with a learning rate 1,000x smaller than the original.\n",
    "\n",
    "Obvs better to use a model pretrained on a similar dataset.\n",
    "\n",
    "Both TF and PyTorch have loads of info about fine-tuning.\n",
    "\n",
    "### Increase # Training Images\n",
    "\n",
    "Use image augmentation.\n",
    "\n",
    "If you rotate images by 180 degrees, you can double the number of training images you have.\n",
    "\n",
    "Not as good as getting new data but way better than nothing. Can add\n",
    "- Crops\n",
    "- Rotation\n",
    "- Adding noise\n",
    "\n",
    "Reduces overfitting and gives more robust models.\n",
    "\n",
    "Can be used on both training and test data. For the latter we can average the predictions for one augmented sample to reduce variance.\n",
    "\n",
    "Must be careful though. Can't just randomly augemt willy nilly. For example in roof classification one of the categories was a North-South orientation, so rotating 90 degrees would turn this image into an East-West orientation image.\n",
    "\n",
    "If fine-tuning (or training from scratch) you must use labels from images in the trained set. Be careful with validation here as overfitting can easily occur. Not sure what he means by this.\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. Use pretrained CNNs to extract features (depending on original training data, we will favour different layers e.g. earlier or later)\n",
    "2. Careful choosing of pretrained network can help (is it originally trained on a similar task to the competition?)\n",
    "3. Finetuning lets us refine pretrained models\n",
    "4. Data augmentation is massively helpful\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-tract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-bronze",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-forest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-armor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-milan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-integral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-declaration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-adaptation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-fence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-dynamics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
