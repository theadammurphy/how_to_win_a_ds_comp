{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fewer-distinction",
   "metadata": {},
   "source": [
    "# Ensemble \n",
    "\n",
    "## Bagging\n",
    "\n",
    "It _always_ makes sense to do at least bagging. This is always going to be more reliable than just building one model.\n",
    "\n",
    "Bagging means **averaging** slightly different versions of the same model to improve accuracy e.g. random forest builds many decision trees.\n",
    "\n",
    "There are 2 main sources of errors when modelling:\n",
    "1. Errors due to bias (underfitting)\n",
    "2. Errors due to variance (overfitting)\n",
    "\n",
    "High bias = not a very deep relationship. We are _biased_ to the data that we have got and have made some wild and sweeping generalizations based off of it.\n",
    "\n",
    "High variance = easily moved by small changes in the data, perhaps we have taken in too much data e.g. seeing if someone will buy a car we do not need to know their eye color or the color of their house.\n",
    "\n",
    "We make slightly different models.\n",
    "\n",
    "The guy teaching this part of the course (and ex-number 1 ranked Kaggler) said he never just fits a model but always uses Bagging. There is no point just fitting one model.\n",
    "\n",
    "Ahhhh shit, wait I've just thought I can manually create bagged GBDTs and LightGBM models myself. I just build say 20 of them with different random seeds and then write the prediction function myself. I don't just have to use the sklearn API to make predictions for me!! Light bulb moment!\n",
    "\n",
    "### Parameters That Control Bagging\n",
    "\n",
    "- Random seed\n",
    "- Row (sub) sampling or bootstrapping\n",
    "- Shuffling - some models will produce different results if the data is presented in a different order\n",
    "- Column (sub) sampling\n",
    "- Model-specific parameters e.g. change regularization strength in LogReg models.\n",
    "- Bags (the number of models) - usually at least 10\n",
    "- (Optional) parallelism\n",
    "\n",
    "Sub-sampling means training models with less data. When we combine these models that have been trained on completely different datasets, they can be amazingly powerful. \n",
    "\n",
    "Bootstrapping is where we randomly build a new dataset _with replacement_ so the model will almost certainly see some data points more than once and it lets us artificially build a bigger dataset.\n",
    "\n",
    "Woah. So yeah there are millions of things we could try.\n",
    "\n",
    "In principle, more bags will never hurt you and will increase performance. But, after some point, you will start plateauing so there is a cost-benefit with time. \n",
    "\n",
    "With bagging, all models are completely independent of each other so you can make full use of all the cores in your machine.\n",
    "\n",
    "The `BaggingClassifier` and `BaggingRegressor` from sklean are both good.\n",
    "\n",
    "Some typical bagging code. I will defo implement this!\n",
    "\n",
    "```python\n",
    "# train is the training data\n",
    "# test is the test data\n",
    "# y is the target variable\n",
    "model = RandomForestRegressor()\n",
    "# Specify bagging params\n",
    "bags = 10\n",
    "seed = 1\n",
    "# create array object to hold bagged predictions\n",
    "bagged_prediction = np.zeros(test.shape[0])\n",
    "# loop for as mind times as we want bags\n",
    "for n in range(0, bags):\n",
    "    model.set_params(random_state=seed + n) # update seed\n",
    "    model.fit(train, y) # fit model\n",
    "    preds = model.predict(test) # predict on test data\n",
    "    bagged_prediction += preds\n",
    "# take average of predictions\n",
    "bagged_prediction /= bags\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-contents",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "A form of weighted averaging of models where each model is built sequentially by taking into account past model performance.\n",
    "\n",
    "Unlike bagging (which just builds loads of independent models), this iterates on one particular model over and over.\n",
    "\n",
    "**Main Boosting Types*\n",
    "1. Weight based\n",
    "2. Residual based\n",
    "\n",
    "Weight can be caluclated based off of the 1 + absolute error between predictions and target. There are many ways to do it but this is just an example.\n",
    "\n",
    "The next model will then be fed the same features and target variable but also an additional `weight` column. The weight will be bigger if there was a bigger error. Thus we give more significance to data points the model cannot classify.\n",
    "\n",
    "### Weight Based Boosting Parameters\n",
    "\n",
    "- Learning rate (or shrinkage or eta)\n",
    " `pred_n = pred_0 * eta + pred_1 * eta + ... + pred_n * eta`. \n",
    " The learning rate ensures that we don't trust one model too much. We trust many models a little bit (important to control overfitting)\n",
    "- Num. estimators - often an inverse relationship with the learning rate. If we have more estimators, we need a smaller learning rate.\n",
    "- Input model - can be anything that accepts weights.\n",
    "- Sub boosting type\n",
    " - AdaBoost - good implementation in sklearn\n",
    " - LogitBoost - good implementation in Weka (Java)\n",
    "\n",
    "To find optimal LR and num estimators, use CV. Start with fixed number of estimators (e.g. 100) and find the optimal LR for that (with CV). Then if we double the estimators, we should halve the learning rate. This gives us solid ballpark figures for how to increase/decrease estimators and LR. Can lose a lot of time if you don't do this as you will faff around trying to find the best LR. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-eagle",
   "metadata": {},
   "source": [
    "## Residual Based Boosting\n",
    "\n",
    "This has been the most dominant and winning-est algorithm for the last years. \n",
    "\n",
    "For first model, you cal predictions. Then you calculate the error (not the absolute error, just the difference) between preds and y. Then you train the next model on the same features but use the error from model one as the target variable.\n",
    "\n",
    "To get predictions for one row, you then add up all the error values on that row. Super clever!\n",
    "\n",
    "### Residual Boosting Parameters\n",
    "\n",
    "- Learning rate (or shrinkage or eta)\n",
    "- Number of estimators\n",
    "- Row (sub) sampling\n",
    "- Column (sub) sampling\n",
    "- Input model - can theoretically be done with anything but best performance so far has been with trees\n",
    "- Sub boosting type (two most common below)\n",
    " - Fully gradient based\n",
    " - DART (particularly good with classification)\n",
    "\n",
    "If the error for this model is 0.2 and our LR is 0.1, it means we only adjust the model prediction by 10%. So the new prediction is `old_pred + 0.2 * 0.1 = old_pred + 0.02`.\n",
    "\n",
    "Normally more estimators is more but you need to offset this with the right learning rate to ensure each model has the right contribution. A high number of estimators mean you need a very small LR.\n",
    "\n",
    "### Excellent Residual Based Boosting Implementations\n",
    "\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- H20's GBM\n",
    "- CatBoost\n",
    "- Sklearn's GBM - can use _any_ sklearn estimator as a base with this one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-wrong",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Super popular form. In _all_ competitions, you will need to use stacking in the end to boost your performance as much as possible.\n",
    "\n",
    "**Definition**: Making predictions with hold-out data sets and then collecting (stacking) these predictions to form a new dataset which you will fit a new model on to make the final predictions.\n",
    "\n",
    "**Methodology**\n",
    "1. Split the train set into two disjoint sets\n",
    "2. Train several base learners on the first part\n",
    "3. Make predictions with the base learners on the second (validation) part\n",
    "4 Use the predictions from 3 as the inputs to train a higher level learner.\n",
    "\n",
    "We call the first models base models/learners and the latter we call meta models.\n",
    "\n",
    "Excellent explanation in the [video](https://www.coursera.org/learn/competitive-data-science/lecture/Qdtt6/stacking). See image below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-turkish",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"stacking.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-bachelor",
   "metadata": {},
   "source": [
    "Note that it's called stacking because we stack the predictions next to each other to form the new datasets B1 and C1.\n",
    "\n",
    "Let's do a code example\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train is the training data\n",
    "# y is the target variable for the training data\n",
    "# test is the test data\n",
    "\n",
    "training, valid, ytraining, yvalid = train_test_split(\n",
    "                                        train, y,\n",
    "                                        test_size=0.5)\n",
    "# Specify models\n",
    "model_1 = RandomForestRegressor()\n",
    "model_2 = LinearRegression()\n",
    "# Fit models\n",
    "model_1.fit(training, ytraining)\n",
    "model_2.fit(training, ytraining)\n",
    "# Make predictions for validation\n",
    "preds_1 = model_1.predict(valid)\n",
    "preds_2 = model_2.predict(valid)\n",
    "# Make predictions for test data\n",
    "test_preds_1 = model_1.predict(test)\n",
    "test_preds_2 = model_2.predict(test)\n",
    "# Form a new dataset for valid and test by stacking\n",
    "# the predictions\n",
    "stacked_predictions = np.column_stack((preds_1, preds_2))\n",
    "stacked_test_predictions = np.column_stack((test_preds_1, test_preds_2))\n",
    "# Specify meta model\n",
    "meta_model = LinearRegression()\n",
    "# Fit meta model on stacked predictions\n",
    "meta_model.fit(stacked_predictions, yvalid)\n",
    "# Make predictions on the stacked predictions of the test data\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)\n",
    "```\n",
    "\n",
    "Seems like a lot but when you read through it, it's actually quite simple.\n",
    "\n",
    "Woah. This is a crazily powerful model. \n",
    "\n",
    "### Things To Be Mindful Of\n",
    "\n",
    "- With time sensitive data, respect time i.e. train in the past, val and test in future\n",
    "- Diversity is as important as performance\n",
    "- Diversity comes from\n",
    " - Different algorithms\n",
    " - Different input features - e.g less features or completely different transformations of the input data e.g. OHE categorical features in one and label encode in the other\n",
    "- Performance plateauing after N models\n",
    "- Meta model is usually modest\n",
    "\n",
    "Stacking is able to get the juice out of all of the models you put in. It is great if you combine loads of different models. Adding in weaker performing models will actually give stacking new features to work with as these models will probably be good in areas where the top performing models are poor. Combining 'weak' leaners with the strong ones is going to make a super crazy strong learner.\n",
    "\n",
    "Can't know beforehand when we will start plateauing but generally it is affected by how many features you have in your data, how much diversity you have included, how many rows of data you have. Tough to know beforehand. But basically just add models and think of new things to try until you cannot get any more value.\n",
    "\n",
    "Meta model is basically only using predictions of the other models. The other models have done the deep work, thus the meta model doesn't have to be that deep. Normally you have predictions that are correlated with the target, you just need to find a way to combine them. So if you use RandomForest, you would use a lower depth than the best one you found in your base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-sleep",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-chemistry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-admission",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-bristol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-ozone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-liberal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-finland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-costs",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serve",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
